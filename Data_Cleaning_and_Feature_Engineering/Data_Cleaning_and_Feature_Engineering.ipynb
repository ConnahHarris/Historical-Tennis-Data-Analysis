{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c45391-7a5b-4170-9cbe-06904ce86986",
   "metadata": {},
   "source": [
    "# Concatinating .csv Files and Creating New df_subset with Improved Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73810c2-b73e-48a3-8e2d-ad414775797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Reads all csv files in this folder and concatenates them\n",
    "csv_files = glob.glob('*.csv')\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Dictionary of column names to change\n",
    "cols_to_rename = {\n",
    "    \"tourney_id\": \"tournament_id\",\n",
    "    \"tourney_name\": \"tournament_name\",\n",
    "    \"tourney_date\": \"tournament_date\",\n",
    "    \"winner_ht\": \"p1_height\",\n",
    "    \"winner_id\": \"p1_id\",\n",
    "    \"winner_name\": \"p1_name\",\n",
    "    \"winner_age\": \"p1_age\",\n",
    "    \"loser_age\": \"p2_age\", \n",
    "    \"winner_hand\": \"p1_hand\",\n",
    "    \"loser_ht\": \"p2_height\",\n",
    "    \"loser_id\": \"p2_id\",\n",
    "    \"loser_name\": \"p2_name\",\n",
    "    \"loser_hand\": \"p2_hand\",\n",
    "    \"tourney_level\": \"tournament_level\",\n",
    "}\n",
    "\n",
    "# Rename useful columns\n",
    "df.rename(columns=cols_to_rename, inplace=True)\n",
    "\n",
    "# remove Round Robin (RR) and Bronze medal (BR) round rows\n",
    "df = df.loc[~df['round'].isin(['RR', 'BR'])].copy()\n",
    "\n",
    "# Change format of tournament date column\n",
    "df['tournament_date'] = pd.to_datetime(df['tournament_date'], format='%Y%m%d')\n",
    "\n",
    "# Define round order\n",
    "round_order = [\"Q1\", \"Q2\", \"Q3\", \"R128\", \"R64\", \"R32\", \"R16\", \"QF\", \"SF\", \"F\"]\n",
    "\n",
    "# Create an ordered categorical column for tournament round\n",
    "df[\"round\"] = pd.Categorical(df[\"round\"], categories=round_order, ordered=True)\n",
    "\n",
    "# Build the list of sort keys that are actually present, then sort\n",
    "sort_keys = [c for c in [\"tournament_date\", \"tournament_id\", \"round\"] if c in df.columns]\n",
    "df = df.sort_values(sort_keys, kind=\"mergesort\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d71478-eccd-432a-836a-d1ee930d2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset of dataframe with useful columns\n",
    "df_subset = df[['tournament_date','tournament_name', 'tournament_level', 'surface', 'p1_name', 'p1_id', 'p1_age', 'p1_height', 'p1_hand', 'p2_name', 'p2_id', 'p2_age', 'p2_height', 'p2_hand']]\n",
    "df_subset = df_subset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa7f05-850e-4e04-ab46-f3db7e267415",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa04c9-e75d-4dd4-a613-3b7e9847bbae",
   "metadata": {},
   "source": [
    "### Looking for Anomalous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49685d-9975-4045-a207-b0f5cb470df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays number of rows in a dataframe\n",
    "len(df_subset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0243336-1a53-4a68-85a3-10db6d9d7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displayes number of NaN values for each column\n",
    "df_subset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb2b06-09d7-4b40-a97c-dab873befb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf05f4b6-66f2-4dfe-a189-fc718330333d",
   "metadata": {},
   "source": [
    "## Anomolous Surface Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1e14b-3aee-4a89-8271-4dd17d9b5b86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subset[df_subset['surface'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e93dc-3ad4-4e12-bbeb-da926301f61a",
   "metadata": {},
   "source": [
    "###### There are no rows with missing values in the surface column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055754b-d332-46ba-b759-0c1ba8fe1bc1",
   "metadata": {},
   "source": [
    "## Anomolous Age Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032c86c-63a3-4b32-82df-df8b2e5457a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subset[(df_subset['p1_age'].isna()) | (df_subset['p2_age'].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052a3e3-b209-475e-a7cc-06047ea3a791",
   "metadata": {},
   "source": [
    "###### All of the players that have missing p1_age and p2_age values also have missing height data. I can not find these player's ages or heights on the ATP website. There are a total of 115 missing age entries in a data frame of 143272 so these will be removed from the df_subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbdbba-de66-44ae-abf4-8a56ef4c975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['p1_age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fabb87-5a65-4371-af98-5575536ba14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['p2_age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9735665-f178-42f0-9061-236d773be178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subset[(df_subset['p1_age'] < 16) | (df_subset['p2_age'] < 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b394ea-a76d-47a1-b574-8c6617bc1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset[(df_subset['p1_age'] > 45) | (df_subset['p2_age'] > 45)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff51f6-0e32-4116-891a-4db297a93de2",
   "metadata": {},
   "source": [
    "###### I have checked for anomalously old and young players. All of the very young and old players listed in the dataset have their correct ages listed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b5f22e-363c-4140-ab92-0eecd67a256f",
   "metadata": {},
   "source": [
    "## Anomolous Height Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a2649-1e1e-4d72-81d7-0b04e0346f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of rows with missing height data for eiter p1 or p2 \n",
    "len(df_subset[(df_subset['p1_height'].isna()) | (df_subset['p2_height'].isna())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81976547-decb-4e04-a936-127c9943a3b0",
   "metadata": {},
   "source": [
    "###### There are 17296 rows in the dataset with missing height data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4c8c4-1201-42b6-8e6b-ddca33f04af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create list of players with missing height data\n",
    "missing_height_p1_names = df_subset[df_subset['p1_height'].isna()]['p1_name']\n",
    "missing_height_p2_names = df_subset[df_subset['p2_height'].isna()]['p2_name']\n",
    "missing_height_names = set(missing_height_p1_names) | set(missing_height_p2_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15efd9b-c9b6-43cb-b21b-fce654131a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8927b-80c7-415b-9cca-9d7d31b32c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wptools\n",
    "import re\n",
    "\n",
    "def get_heights(names): \n",
    "    # store results as name: height_cm\n",
    "    heights = {}  \n",
    "    \n",
    "    for name in names:\n",
    "        try:\n",
    "            page = wptools.page(name, silent=True).get_parse() \n",
    "            infobox = page.data.get('infobox', {}) \n",
    "            height_raw = infobox.get('height', '') \n",
    "            \n",
    "            # Extract height in meters from the wikipedia page\n",
    "            match = re.search(r'\\{\\{height\\|m\\|=\\|([\\d.]+)\\}\\}', height_raw) \n",
    "            if match: \n",
    "                height_cm = float(match.group(1)) * 100\n",
    "                heights[name] = height_cm\n",
    "            else:\n",
    "                # If it could not be parsed\n",
    "                heights[name] = None  \n",
    "        except Exception:\n",
    "            # page not found or other error\n",
    "            heights[name] = None  \n",
    "    \n",
    "    return heights\n",
    "\n",
    "#get_heights(missing_height_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b2a70-25fd-4903-bd93-03be9766e3b1",
   "metadata": {},
   "source": [
    "###### I have searched wikipedia for their height data to amend the entries, but these players do not have wikipedia pages. The NaN results will not show in later in seaborn plots, however these results will cause errors later on when trying to train a model on this data so they will be removed. There are 17296 rows being deleted which may impact ELO calculations, therefore I will remove these rows after the ELOs have been calculated.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5002c7-e96e-4543-b303-d19bed1aba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['p1_height'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20825fd-fc58-4e9e-88e5-227a2589bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['p2_height'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95255be5-9420-4355-8101-b479d9e3333c",
   "metadata": {},
   "source": [
    "###### In both p1 and p2 height columns, there is a player listed as being 3 cm tall, this is clearly anomolous data so I will find their names and try to amend their height data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cfaa2a-c2e9-4f13-8cc4-b4d9864f6b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for anomolously short players\n",
    "df_subset[(df_subset['p1_height'] <= 160) | (df_subset['p2_height'] <= 160)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01988c21-1090-433e-a636-34398bee5af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for anomolously tall players\n",
    "df_subset[(df_subset['p1_height'] >= 211) | (df_subset['p2_height'] >= 211)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0598a-2f86-4820-b46e-c8f7e8fdd9f5",
   "metadata": {},
   "source": [
    "###### I looked up the players with anonomous height data i.e. height = 3 cm, their heights are not shown on the ATP website so they will be removed after calculating ELOs etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef3c878-8d09-4d5a-804a-3676484a8890",
   "metadata": {},
   "source": [
    "## Missing Player Hand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd8aa0-8eee-469c-8c31-5d3209e3b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset[(df_subset['p1_hand'] == 'U') | (df_subset['p2_hand'] == 'U')].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399551fd-9246-44f3-adda-1c59bdccf85c",
   "metadata": {},
   "source": [
    "###### Players with unknown handedness usually have other missing data so they will be removed after calculating ELOs etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ea21b-a1d4-4d84-92fa-7f0ece252b79",
   "metadata": {},
   "source": [
    "# Calculating New Features for Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74880549-26bf-498f-a4d7-91ff9e09e641",
   "metadata": {},
   "source": [
    "## Calculating Age Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919628f-23a3-4d32-a063-a16391d642cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store age differences\n",
    "p1_age_diff_list = []\n",
    "p2_age_diff_list = []\n",
    "\n",
    "# loops over the age columns and calculates players age difference\n",
    "for p1, p2 in zip(df_subset['p1_age'], df_subset['p2_age']):\n",
    "    p1_age_diff = p1 - p2\n",
    "    p2_age_diff = p2 - p1\n",
    "\n",
    "    # Adds calculated age differences to list\n",
    "    p1_age_diff_list.append(p1_age_diff)\n",
    "    p2_age_diff_list.append(p2_age_diff)\n",
    "\n",
    "# Assign lists to new df columns\n",
    "df_subset['p1_age_diff'] = p1_age_diff_list\n",
    "df_subset['p2_age_diff'] = p2_age_diff_list\n",
    "df_subset['p1_age_diff'] = df_subset['p1_age_diff'].round(1)\n",
    "df_subset['p2_age_diff'] = df_subset['p2_age_diff'].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6906f3-77fb-42d8-b65a-734766dbeb75",
   "metadata": {},
   "source": [
    "## Calculating Height Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef1004-fcc4-4bb0-943f-1271af9e43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store height differences\n",
    "p1_height_diff_list = []\n",
    "p2_height_diff_list = []\n",
    "\n",
    "# loops over the height columns and calculates players height difference\n",
    "for p1, p2 in zip(df_subset['p1_height'], df_subset['p2_height']):\n",
    "    p1_height_diff = p1 - p2\n",
    "    p2_height_diff = p2 - p1\n",
    "\n",
    "    # Adds calculated age differences to list\n",
    "    p1_height_diff_list.append(p1_height_diff)\n",
    "    p2_height_diff_list.append(p2_height_diff)\n",
    "\n",
    "# Assign lists to new df columns\n",
    "df_subset['p1_height_diff'] = p1_height_diff_list\n",
    "df_subset['p2_height_diff'] = p2_height_diff_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58bfc48-1ef9-47c9-b3b8-ab8fdfeacf37",
   "metadata": {},
   "source": [
    "# Calculating Previous H2H Wins Against Opponent Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e66ac-f7f3-4dda-a87a-56f9e2d690da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "# Dictionary that stores head-to-head match results. Returns 0 if the key does not exist (players have never played before)\n",
    "h2h_wins_dict = defaultdict(int)\n",
    "\n",
    "# Dictionary to store sequence of winners of matches between players\n",
    "h2h_history_dict = defaultdict(list)\n",
    "\n",
    "# Stores total h2h wins before the current match\n",
    "p1_h2h_wins_before = []\n",
    "p2_h2h_wins_before = []\n",
    "\n",
    "# Stores total h2h win difference before the current match\n",
    "p1_h2h_wins_total_diff_before = []\n",
    "p2_h2h_wins_total_diff_before = []\n",
    "\n",
    "# Stores h2h win difference in last game\n",
    "p1_h2h_wins_last1_diff_before = []\n",
    "p2_h2h_wins_last1_diff_before = []\n",
    "\n",
    "# Stores h2h win difference in last 2 games\n",
    "p1_h2h_wins_last2_diff_before = []\n",
    "p2_h2h_wins_last2_diff_before = []\n",
    "\n",
    "# Stores h2h win difference in last 3 games\n",
    "p1_h2h_wins_last3_diff_before = []\n",
    "p2_h2h_wins_last3_diff_before = []\n",
    "\n",
    "# Stores h2h win difference in last 4 games\n",
    "p1_h2h_wins_last4_diff_before = []\n",
    "p2_h2h_wins_last4_diff_before = []\n",
    "\n",
    "# Stores h2h win difference in last 5 games\n",
    "p1_h2h_wins_last5_diff_before = []\n",
    "p2_h2h_wins_last5_diff_before = []\n",
    "\n",
    "# Stores h2h win difference in last 10 games\n",
    "p1_h2h_wins_last10_diff_before = []\n",
    "p2_h2h_wins_last10_diff_before = []\n",
    "\n",
    "\n",
    "# Iterate through each match in df_subset, returning player_ids as pairs  \n",
    "for p1, p2 in zip(df_subset['p1_id'], df_subset['p2_id']):\n",
    "    \n",
    "    # Creates keys for head-to-head matches\n",
    "    wins_key1 = (p1, p2)\n",
    "    wins_key2 = (p2, p1)\n",
    "    wins_match_key = tuple(sorted([p1, p2]))\n",
    "\n",
    "    # Get total wins for each player before this match\n",
    "    p1_h2h_wins = h2h_wins_dict[wins_key1]\n",
    "    p2_h2h_wins = h2h_wins_dict[wins_key2]\n",
    "    \n",
    "    # Saves wins to respective p1_h2h_wins_before and p2_h2h_wins_before lists to then be used for dataframe columns\n",
    "    p1_h2h_wins_before.append(p1_h2h_wins)\n",
    "    p2_h2h_wins_before.append(p2_h2h_wins)\n",
    "    \n",
    "    # Calculates wins difference\n",
    "    p1_h2h_wins_diff = p1_h2h_wins - p2_h2h_wins\n",
    "    p2_h2h_wins_diff = p2_h2h_wins - p1_h2h_wins\n",
    "    \n",
    "    # Saves differences to px_h2h_wins_diff_before lists to then be used for dataframe columns\n",
    "    p1_h2h_wins_total_diff_before.append(p1_h2h_wins_diff)\n",
    "    p2_h2h_wins_total_diff_before.append(p2_h2h_wins_diff)\n",
    "\n",
    "    # Get last 1 match results and compute difference\n",
    "    history_last1 = h2h_history_dict[wins_match_key][-1:]\n",
    "    p1_last1_wins = history_last1.count(p1)\n",
    "    p2_last1_wins = history_last1.count(p2)\n",
    "    p1_last1_diff = p1_last1_wins - p2_last1_wins\n",
    "    p2_last1_diff = p2_last1_wins - p1_last1_wins\n",
    "    p1_h2h_wins_last1_diff_before.append(p1_last1_diff)\n",
    "    p2_h2h_wins_last1_diff_before.append(p2_last1_diff)\n",
    "\n",
    "    # Get last 2 match results and compute difference\n",
    "    history_last2 = h2h_history_dict[wins_match_key][-2:]\n",
    "    p1_last2_wins = history_last2.count(p1)\n",
    "    p2_last2_wins = history_last2.count(p2)\n",
    "    p1_last2_diff = p1_last2_wins - p2_last2_wins\n",
    "    p2_last2_diff = p2_last2_wins - p1_last2_wins\n",
    "    p1_h2h_wins_last2_diff_before.append(p1_last2_diff)\n",
    "    p2_h2h_wins_last2_diff_before.append(p2_last2_diff)\n",
    "\n",
    "    # Get last 3 match results and compute difference\n",
    "    history_last3 = h2h_history_dict[wins_match_key][-3:]\n",
    "    p1_last3_wins = history_last3.count(p1)\n",
    "    p2_last3_wins = history_last3.count(p2)\n",
    "    p1_last3_diff = p1_last3_wins - p2_last3_wins\n",
    "    p2_last3_diff = p2_last3_wins - p1_last3_wins\n",
    "    p1_h2h_wins_last3_diff_before.append(p1_last3_diff)\n",
    "    p2_h2h_wins_last3_diff_before.append(p2_last3_diff)\n",
    "\n",
    "    # Get last 4 match results and compute difference\n",
    "    history_last4 = h2h_history_dict[wins_match_key][-4:]\n",
    "    p1_last4_wins = history_last4.count(p1)\n",
    "    p2_last4_wins = history_last4.count(p2)\n",
    "    p1_last4_diff = p1_last4_wins - p2_last4_wins\n",
    "    p2_last4_diff = p2_last4_wins - p1_last4_wins\n",
    "    p1_h2h_wins_last4_diff_before.append(p1_last4_diff)\n",
    "    p2_h2h_wins_last4_diff_before.append(p2_last4_diff)\n",
    "\n",
    "    # Get last 5 match results and compute difference\n",
    "    history_last5 = h2h_history_dict[wins_match_key][-5:]\n",
    "    p1_last5_wins = history_last5.count(p1)\n",
    "    p2_last5_wins = history_last5.count(p2)\n",
    "    p1_last5_diff = p1_last5_wins - p2_last5_wins\n",
    "    p2_last5_diff = p2_last5_wins - p1_last5_wins\n",
    "    p1_h2h_wins_last5_diff_before.append(p1_last5_diff)\n",
    "    p2_h2h_wins_last5_diff_before.append(p2_last5_diff)\n",
    "\n",
    "    # Get last 10 match results and compute difference\n",
    "    history_last10 = h2h_history_dict[wins_match_key][-10:]\n",
    "    p1_last10_wins = history_last10.count(p1)\n",
    "    p2_last10_wins = history_last10.count(p2)\n",
    "    p1_last10_diff = p1_last10_wins - p2_last10_wins\n",
    "    p2_last10_diff = p2_last10_wins - p1_last10_wins\n",
    "    p1_h2h_wins_last10_diff_before.append(p1_last10_diff)\n",
    "    p2_h2h_wins_last10_diff_before.append(p2_last10_diff)\n",
    "\n",
    "    # Player 1 always wins in this df_subset, updates head-to-head\n",
    "    h2h_wins_dict[wins_key1] += 1  \n",
    "\n",
    "    # Updates h2h history dicitonary \n",
    "    h2h_history_dict[wins_match_key].append(p1)\n",
    "\n",
    "# Assign to dataframe\n",
    "df_subset['p1_h2h_wins'] = p1_h2h_wins_before\n",
    "df_subset['p2_h2h_wins'] = p2_h2h_wins_before\n",
    "df_subset['p1_h2h_wins_before_total_diff'] = p1_h2h_wins_total_diff_before\n",
    "df_subset['p2_h2h_wins_before_total_diff'] = p2_h2h_wins_total_diff_before\n",
    "df_subset['p1_h2h_wins_before_last1_diff'] = p1_h2h_wins_last1_diff_before\n",
    "df_subset['p2_h2h_wins_before_last1_diff'] = p2_h2h_wins_last1_diff_before\n",
    "df_subset['p1_h2h_wins_before_last2_diff'] = p1_h2h_wins_last2_diff_before\n",
    "df_subset['p2_h2h_wins_before_last2_diff'] = p2_h2h_wins_last2_diff_before\n",
    "df_subset['p1_h2h_wins_before_last3_diff'] = p1_h2h_wins_last3_diff_before\n",
    "df_subset['p2_h2h_wins_before_last3_diff'] = p2_h2h_wins_last3_diff_before\n",
    "df_subset['p1_h2h_wins_before_last4_diff'] = p1_h2h_wins_last4_diff_before\n",
    "df_subset['p2_h2h_wins_before_last4_diff'] = p2_h2h_wins_last4_diff_before\n",
    "df_subset['p1_h2h_wins_before_last5_diff'] = p1_h2h_wins_last5_diff_before\n",
    "df_subset['p2_h2h_wins_before_last5_diff'] = p2_h2h_wins_last5_diff_before\n",
    "df_subset['p1_h2h_wins_before_last10_diff'] = p1_h2h_wins_last10_diff_before\n",
    "df_subset['p2_h2h_wins_before_last10_diff'] = p2_h2h_wins_last10_diff_before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2a19a-0481-4ce4-b21e-ef35ebc098dd",
   "metadata": {},
   "source": [
    "# Calculating ELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab316383-a868-4c0f-a9e0-a559d9e0bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Dictionary that stores players ELO. Returns 1500 if the key does not exist (player has not played before)\n",
    "elo_dict = defaultdict(lambda: 1500)\n",
    "\n",
    "# Stores immediate elo\n",
    "p1_elo_before = []\n",
    "p2_elo_before = []\n",
    "\n",
    "# Iterate through each match in df_subset, returning player_ids as pairs \n",
    "for p1, p2 in zip(df_subset['p1_id'], df_subset['p2_id']):\n",
    "    elo_key1 = p1\n",
    "    elo_key2 = p2\n",
    "\n",
    "    # Checks dictionary for ELO and stores the ELO as px_elo\n",
    "    p1_elo = elo_dict[elo_key1]\n",
    "    p2_elo = elo_dict[elo_key2]\n",
    "\n",
    "    # Adds ELO to the list px_elo_before\n",
    "    p1_elo_before.append(p1_elo)\n",
    "    p2_elo_before.append(p2_elo)\n",
    "\n",
    "    # Calculates expected score\n",
    "    p1_expected_score = 1 / (1 + 10**((p2_elo - p1_elo)/400))\n",
    "    p2_expected_score = 1 / (1 + 10**((p1_elo - p2_elo)/400))\n",
    "\n",
    "    # Calculates ELO after the match\n",
    "    K = 32\n",
    "    p1_elo_after = int(p1_elo + K * (1 - p1_expected_score))\n",
    "    p2_elo_after = int(p2_elo + K * (0 - p2_expected_score))\n",
    "\n",
    "    # Stores new ELO in the dictionary\n",
    "    elo_dict[elo_key1] = p1_elo_after\n",
    "    elo_dict[elo_key2] = p2_elo_after\n",
    "    \n",
    "# Assign to dataframe\n",
    "df_subset['p1_elo_before'] = p1_elo_before\n",
    "df_subset['p2_elo_before'] = p2_elo_before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e22466a-0cbd-4f65-bb75-5d6a905e6cfd",
   "metadata": {},
   "source": [
    "#### Calculating Rolling ELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0fee7-5f61-440e-b1df-d42131924847",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['p1_elo_rolling_last5'] = (df_subset.groupby('p1_id')['p1_elo_before'].transform(lambda x: x.shift().rolling(5, min_periods=1).mean())).fillna(1500).astype(int)\n",
    "df_subset['p2_elo_rolling_last5'] = (df_subset.groupby('p2_id')['p2_elo_before'].transform(lambda x: x.shift().rolling(5, min_periods=1).mean())).fillna(1500).astype(int)\n",
    "df_subset['p1_elo_rolling_last10'] = (df_subset.groupby('p1_id')['p1_elo_before'].transform(lambda x: x.shift().rolling(10, min_periods=1).mean())).fillna(1500).astype(int)\n",
    "df_subset['p2_elo_rolling_last10'] = (df_subset.groupby('p2_id')['p2_elo_before'].transform(lambda x: x.shift().rolling(10, min_periods=1).mean())).fillna(1500).astype(int)\n",
    "df_subset['p1_elo_rolling_last20'] = (df_subset.groupby('p1_id')['p1_elo_before'].transform(lambda x: x.shift().rolling(20, min_periods=1).mean())).fillna(1500).astype(int)\n",
    "df_subset['p2_elo_rolling_last20'] = (df_subset.groupby('p2_id')['p2_elo_before'].transform(lambda x: x.shift().rolling(20, min_periods=1).mean())).fillna(1500).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b0228-848a-4b2d-9241-fa6a4ea14a1d",
   "metadata": {},
   "source": [
    "#### Calculating Rolling ELO difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db077b-6a73-4406-809d-744170ca3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store rolling ELO differences\n",
    "p1_elo_rolling_last5_diff_list = []\n",
    "p2_elo_rolling_last5_diff_list = []\n",
    "\n",
    "p1_elo_rolling_last10_diff_list = []\n",
    "p2_elo_rolling_last10_diff_list = []\n",
    "\n",
    "p1_elo_rolling_last20_diff_list = []\n",
    "p2_elo_rolling_last20_diff_list = []\n",
    "\n",
    "# Loops over the last5 rolling ELO columns and calculates the players ELO difference\n",
    "for p1, p2 in zip(df_subset['p1_elo_rolling_last5'], df_subset['p2_elo_rolling_last5']):\n",
    "    p1_elo_rolling_last5_diff = p1 - p2\n",
    "    p2_elo_rolling_last5_diff = p2 - p1\n",
    "\n",
    "    # Adds calculated ELO differences to list\n",
    "    p1_elo_rolling_last5_diff_list.append(p1_elo_rolling_last5_diff)\n",
    "    p2_elo_rolling_last5_diff_list.append(p2_elo_rolling_last5_diff)\n",
    "\n",
    "# Loops over the last10 rolling ELO columns and calculates the players ELO difference\n",
    "for p1, p2 in zip(df_subset['p1_elo_rolling_last10'], df_subset['p2_elo_rolling_last10']):\n",
    "    p1_elo_rolling_last10_diff = p1 - p2\n",
    "    p2_elo_rolling_last10_diff = p2 - p1\n",
    "\n",
    "    # Adds calculated ELO differences to list\n",
    "    p1_elo_rolling_last10_diff_list.append(p1_elo_rolling_last10_diff)\n",
    "    p2_elo_rolling_last10_diff_list.append(p2_elo_rolling_last10_diff)\n",
    "\n",
    "# Loops over the last20 rolling ELO columns and calculates the players ELO difference\n",
    "for p1, p2 in zip(df_subset['p1_elo_rolling_last20'], df_subset['p2_elo_rolling_last20']):\n",
    "    p1_elo_rolling_last20_diff = p1 - p2\n",
    "    p2_elo_rolling_last20_diff = p2 - p1\n",
    "\n",
    "    # Adds calculated ELO differences to list\n",
    "    p1_elo_rolling_last20_diff_list.append(p1_elo_rolling_last20_diff)\n",
    "    p2_elo_rolling_last20_diff_list.append(p2_elo_rolling_last20_diff)\n",
    "\n",
    "\n",
    "# Assign lists to new df columns\n",
    "df_subset['p1_elo_rolling_last5_diff_before'] = p1_elo_rolling_last5_diff_list\n",
    "df_subset['p2_elo_rolling_last5_diff_before'] = p2_elo_rolling_last5_diff_list\n",
    "df_subset['p1_elo_rolling_last10_diff_before'] = p1_elo_rolling_last10_diff_list\n",
    "df_subset['p2_elo_rolling_last10_diff_before'] = p2_elo_rolling_last10_diff_list\n",
    "df_subset['p1_elo_rolling_last20_diff_before'] = p1_elo_rolling_last20_diff_list\n",
    "df_subset['p2_elo_rolling_last20_diff_before'] = p2_elo_rolling_last20_diff_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388959ed-caf6-4c5a-8897-c63e8982c18d",
   "metadata": {},
   "source": [
    "#### Calculating ELO difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02a69f-57f7-406a-bf24-e123533fafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store ELO differences\n",
    "p1_elo_diff_list = []\n",
    "p2_elo_diff_list = []\n",
    "\n",
    "# Loops over the ELO columns and calculates the players ELO difference\n",
    "for p1, p2 in zip(df_subset['p1_elo_before'], df_subset['p2_elo_before']):\n",
    "    p1_elo_diff = p1 - p2\n",
    "    p2_elo_diff = p2 - p1\n",
    "\n",
    "    # Adds calculated ELO differences to list\n",
    "    p1_elo_diff_list.append(p1_elo_diff)\n",
    "    p2_elo_diff_list.append(p2_elo_diff)\n",
    "\n",
    "# Assign lists to new df columns\n",
    "df_subset['p1_elo_diff_before'] = p1_elo_diff_list\n",
    "df_subset['p2_elo_diff_before'] = p2_elo_diff_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff8fbf-1801-4e68-9f62-416d654cbf66",
   "metadata": {},
   "source": [
    "#### Displays Total Number of Games Played on each Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ebf51-a954-404a-bdd9-e466a68297d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_dict = {}\n",
    "\n",
    "for surface in df_subset['surface']:\n",
    "    surface_dict[surface] = surface_dict.get(surface, 0) + 1\n",
    "\n",
    "print(surface_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6763ccdb-cb08-4446-87d6-cecd452add56",
   "metadata": {},
   "source": [
    "#### Calculating surface ELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c37f6-dfc2-47f3-a7d3-ce510040ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Creates a nested defaultdict: elo_dict[player_id][surface] = ELO\n",
    "elo_dict = defaultdict(lambda: defaultdict(lambda: 1500))\n",
    "\n",
    "# Lists to store ELOs before the match\n",
    "p1_surface_elo_before = []\n",
    "p2_surface_elo_before = []\n",
    "\n",
    "# Loop through df_subset row by row\n",
    "for surface, p1, p2 in zip(df_subset['surface'], df_subset['p1_id'], df_subset['p2_id']):\n",
    "\n",
    "    # Get each player's current ELO on this surface\n",
    "    p1_elo = elo_dict[p1][surface]\n",
    "    p2_elo = elo_dict[p2][surface]\n",
    "\n",
    "    # Store ELOs before match\n",
    "    p1_surface_elo_before.append(p1_elo)\n",
    "    p2_surface_elo_before.append(p2_elo)\n",
    "\n",
    "    # Calculate expected scores\n",
    "    p1_expected = 1 / (1 + 10 ** ((p2_elo - p1_elo) / 400))\n",
    "    p2_expected = 1 / (1 + 10 ** ((p1_elo - p2_elo) / 400))\n",
    "\n",
    "    # Update ELOs assuming p1 wins\n",
    "    K = 32\n",
    "    elo_dict[p1][surface] = int(p1_elo + K * (1 - p1_expected))\n",
    "    elo_dict[p2][surface] = int(p2_elo + K * (0 - p2_expected))\n",
    "\n",
    "# Add columns to df_subset\n",
    "df_subset['p1_surface_elo_before'] = p1_surface_elo_before\n",
    "df_subset['p2_surface_elo_before'] = p2_surface_elo_before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed0010-da6e-4d5e-b9c6-cc9fee07916e",
   "metadata": {},
   "source": [
    "#### Calculating Rolling Surface ELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63411f56-74ca-4498-a5e0-8248d8399f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['p1_surface_elo_rolling_last5'] = (df_subset.groupby('p1_id')['p1_surface_elo_before'].transform(lambda x: x.shift().rolling(5, min_periods=1).mean())).fillna(1500).astype(int)\n",
    "df_subset['p2_surface_elo_rolling_last5'] = (df_subset.groupby('p2_id')['p2_surface_elo_before'].transform(lambda x: x.shift().rolling(5, min_periods=1).mean())).fillna(1500).astype(int)\n",
    "df_subset['p1_surface_elo_rolling_last10'] = (df_subset.groupby('p1_id')['p1_surface_elo_before'].transform(lambda x: x.shift().rolling(10, min_periods=1).mean())).fillna(1500).astype(int)\n",
    "df_subset['p2_surface_elo_rolling_last10'] = (df_subset.groupby('p2_id')['p2_surface_elo_before'].transform(lambda x: x.shift().rolling(10, min_periods=1).mean())).fillna(1500).astype(int)\n",
    "df_subset['p1_surface_elo_rolling_last20'] = (df_subset.groupby('p1_id')['p1_surface_elo_before'].transform(lambda x: x.shift().rolling(20, min_periods=1).mean())).fillna(1500).astype(int)\n",
    "df_subset['p2_surface_elo_rolling_last20'] = (df_subset.groupby('p2_id')['p2_surface_elo_before'].transform(lambda x: x.shift().rolling(20, min_periods=1).mean())).fillna(1500).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af93ed-c131-43e4-be5e-72f5e0afc1b1",
   "metadata": {},
   "source": [
    "#### Calculating Rolling Surface ELO difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19215d-a620-4f4d-8eb1-f6b9450fa8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store rolling ELO differences\n",
    "p1_surface_elo_rolling_last5_diff_list = []\n",
    "p2_surface_elo_rolling_last5_diff_list = []\n",
    "\n",
    "p1_surface_elo_rolling_last10_diff_list = []\n",
    "p2_surface_elo_rolling_last10_diff_list = []\n",
    "\n",
    "p1_surface_elo_rolling_last20_diff_list = []\n",
    "p2_surface_elo_rolling_last20_diff_list = []\n",
    "\n",
    "# Loops over the last5 rolling ELO columns and calculates the players ELO difference\n",
    "for p1, p2 in zip(df_subset['p1_surface_elo_rolling_last5'], df_subset['p2_surface_elo_rolling_last5']):\n",
    "    p1_surface_elo_rolling_last5_diff = p1 - p2\n",
    "    p2_surface_elo_rolling_last5_diff = p2 - p1\n",
    "\n",
    "    # Adds calculated ELO differences to list\n",
    "    p1_surface_elo_rolling_last5_diff_list.append(p1_surface_elo_rolling_last5_diff)\n",
    "    p2_surface_elo_rolling_last5_diff_list.append(p2_surface_elo_rolling_last5_diff)\n",
    "\n",
    "# Loops over the last10 rolling ELO columns and calculates the players ELO difference\n",
    "for p1, p2 in zip(df_subset['p1_surface_elo_rolling_last10'], df_subset['p2_surface_elo_rolling_last10']):\n",
    "    p1_surface_elo_rolling_last10_diff = p1 - p2\n",
    "    p2_surface_elo_rolling_last10_diff = p2 - p1\n",
    "\n",
    "    # Adds calculated ELO differences to list\n",
    "    p1_surface_elo_rolling_last10_diff_list.append(p1_surface_elo_rolling_last10_diff)\n",
    "    p2_surface_elo_rolling_last10_diff_list.append(p2_surface_elo_rolling_last10_diff)\n",
    "\n",
    "# Loops over the last20 rolling ELO columns and calculates the players ELO difference\n",
    "for p1, p2 in zip(df_subset['p1_surface_elo_rolling_last20'], df_subset['p2_surface_elo_rolling_last20']):\n",
    "    p1_surface_elo_rolling_last20_diff = p1 - p2\n",
    "    p2_surface_elo_rolling_last20_diff = p2 - p1\n",
    "\n",
    "    # Adds calculated ELO differences to list\n",
    "    p1_surface_elo_rolling_last20_diff_list.append(p1_surface_elo_rolling_last20_diff)\n",
    "    p2_surface_elo_rolling_last20_diff_list.append(p2_surface_elo_rolling_last20_diff)\n",
    "\n",
    "\n",
    "# Assign lists to new df columns\n",
    "df_subset['p1_surface_elo_rolling_last5_diff_before'] = p1_surface_elo_rolling_last5_diff_list\n",
    "df_subset['p2_surface_elo_rolling_last5_diff_before'] = p2_surface_elo_rolling_last5_diff_list\n",
    "df_subset['p1_surface_elo_rolling_last10_diff_before'] = p1_surface_elo_rolling_last10_diff_list\n",
    "df_subset['p2_surface_elo_rolling_last10_diff_before'] = p2_surface_elo_rolling_last10_diff_list\n",
    "df_subset['p1_surface_elo_rolling_last20_diff_before'] = p1_surface_elo_rolling_last20_diff_list\n",
    "df_subset['p2_surface_elo_rolling_last20_diff_before'] = p2_surface_elo_rolling_last20_diff_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc9c62-2dee-4e06-ba9e-20e99ef48269",
   "metadata": {},
   "source": [
    "#### Calculating surface ELO difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eede08-967d-4ba7-922c-26d67529a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates empty lists to store surface ELO differences\n",
    "p1_surface_elo_diff_list = []\n",
    "p2_surface_elo_diff_list = []\n",
    "\n",
    "# Loops over the surface ELO columns and calculates the surface ELO difference \n",
    "for p1, p2 in zip(df_subset['p1_surface_elo_before'], df_subset['p2_surface_elo_before']):\n",
    "    p1_surface_elo_diff = p1 - p2\n",
    "    p2_surface_elo_diff = p2 - p1\n",
    "\n",
    "    # Adds calculates surface ELO difference to list\n",
    "    p1_surface_elo_diff_list.append(p1_surface_elo_diff)\n",
    "    p2_surface_elo_diff_list.append(p2_surface_elo_diff)\n",
    "\n",
    "# Assign lists to new df columns\n",
    "df_subset['p1_surface_elo_diff_before'] = p1_surface_elo_diff_list\n",
    "df_subset['p2_surface_elo_diff_before'] = p2_surface_elo_diff_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeaeb62-1a48-4a6a-bfb9-14fafd9ae178",
   "metadata": {},
   "source": [
    "# Calculating Total Number of matches a Player Has Played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcaddea-29f7-4951-9c12-f90760b985df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Dictionary to store the number of matches each player has played before\n",
    "total_matches_dict = defaultdict(int)\n",
    "\n",
    "# Empty lists to store total number of matches played by each player \n",
    "p1_total_matches_before = []\n",
    "p2_total_matches_before = []\n",
    "\n",
    "# Loops over player id columns and creates a key based on the players id\n",
    "for p1, p2 in zip(df_subset['p1_id'], df_subset['p2_id']):\n",
    "    total_matches_key1 = p1\n",
    "    total_matches_key2 = p2\n",
    "\n",
    "    # Get current total number of matches for each player before current game\n",
    "    p1_total_matches = total_matches_dict[total_matches_key1]\n",
    "    p2_total_matches = total_matches_dict[total_matches_key2]\n",
    "\n",
    "    # Stores the total number of matches for each player before current game\n",
    "    p1_total_matches_before.append(p1_total_matches)\n",
    "    p2_total_matches_before.append(p2_total_matches)\n",
    "\n",
    "    # Increment the match count for each player\n",
    "    p1_total_matches_after = p1_total_matches + 1\n",
    "    p2_total_matches_after = p2_total_matches + 1\n",
    "\n",
    "    # Update the dictionary with the new total matches played after this match \n",
    "    total_matches_dict[total_matches_key1] = p1_total_matches_after\n",
    "    total_matches_dict[total_matches_key2] = p2_total_matches_after\n",
    "\n",
    "# Assign lists to new df columns\n",
    "df_subset['p1_total_matches_played_before'] = p1_total_matches_before\n",
    "df_subset['p2_total_matches_played_before'] = p2_total_matches_before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4305b6-4b29-407a-a759-0f73b29163b6",
   "metadata": {},
   "source": [
    "#### Calculate total matches player difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72966928-f359-49df-a762-effc27009c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_total_career_matches_diff_list = []\n",
    "p2_total_career_matches_diff_list = []\n",
    "\n",
    "for p1, p2 in zip(df_subset['p1_total_matches_played_before'], df_subset['p2_total_matches_played_before']):\n",
    "    p1_career_matches_diff = p1 - p2\n",
    "    p2_career_matches_diff = p2 - p1\n",
    "\n",
    "    p1_total_career_matches_diff_list.append(p1_career_matches_diff)\n",
    "    p2_total_career_matches_diff_list.append(p2_career_matches_diff)\n",
    "\n",
    "df_subset['p1_total_matches_played_before_diff'] = p1_total_career_matches_diff_list\n",
    "df_subset['p2_total_matches_played_before_diff'] = p2_total_career_matches_diff_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07b01d-3ed6-4acb-b623-9203888f0be3",
   "metadata": {},
   "source": [
    "# Calculating Total Wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eebc64-9404-43eb-84b3-b57655f6c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize dictionaries to track career wins and match outcomes\n",
    "career_wins_dict = defaultdict(int)\n",
    "career_wins_history_dict = defaultdict(list)\n",
    "\n",
    "# Lists to store computed features for Dataframe\n",
    "p1_career_wins_before = []\n",
    "p2_career_wins_before = []\n",
    "\n",
    "# Player win percentage in last 3 matches before current match\n",
    "p1_career_wins_last3_pct_before = []\n",
    "p2_career_wins_last3_pct_before = []\n",
    "\n",
    "# Player win percentage in last 5 matches before current match\n",
    "p1_career_wins_last5_pct_before = []\n",
    "p2_career_wins_last5_pct_before = []\n",
    "\n",
    "# Player win percentage in last 10 matches before current match\n",
    "p1_career_wins_last10_pct_before = []\n",
    "p2_career_wins_last10_pct_before = []\n",
    "\n",
    "# Define time periods for recent win percentages \n",
    "x1 = 3\n",
    "x2 = 5\n",
    "x3 = 10\n",
    "\n",
    "for p1, p2 in zip(df_subset['p1_id'], df_subset['p2_id']):\n",
    "    # Store total wins before this match\n",
    "    p1_career_wins_before.append(career_wins_dict[p1])\n",
    "    p2_career_wins_before.append(career_wins_dict[p2])\n",
    "\n",
    "    # Get last 3 outcomes\n",
    "    p1_last3 = career_wins_history_dict[p1][-x1:]\n",
    "    p2_last3 = career_wins_history_dict[p2][-x1:]\n",
    "\n",
    "    # Get last 5 outcomes\n",
    "    p1_last5 = career_wins_history_dict[p1][-x2:]\n",
    "    p2_last5 = career_wins_history_dict[p2][-x2:]\n",
    "\n",
    "    # Get last 10 outcomes\n",
    "    p1_last10 = career_wins_history_dict[p1][-x3:]\n",
    "    p2_last10 = career_wins_history_dict[p2][-x3:]\n",
    "\n",
    "    # Compute recent win 3 match win percentage\n",
    "    p1_last3_pct = round((sum(p1_last3) / x1) * 100, 1) if len(p1_last3) == x1 else 0\n",
    "    p2_last3_pct = round((sum(p2_last3) / x1) * 100, 1) if len(p2_last3) == x1 else 0\n",
    "\n",
    "    # Compute recent 5 match win percentage\n",
    "    p1_last5_pct = round((sum(p1_last5) / x2) * 100, 1) if len(p1_last5) == x2 else 0\n",
    "    p2_last5_pct = round((sum(p2_last5) / x2) * 100, 1) if len(p2_last5) == x2 else 0\n",
    "\n",
    "    # Compute recent 10 match win percentage\n",
    "    p1_last10_pct = round((sum(p1_last10) / x3) * 100, 1) if len(p1_last10) == x3 else 0\n",
    "    p2_last10_pct = round((sum(p2_last10) / x3) * 100, 1) if len(p2_last10) == x3 else 0\n",
    "\n",
    "    # Append percentages to respective lists\n",
    "    p1_career_wins_last3_pct_before.append(p1_last3_pct)\n",
    "    p2_career_wins_last3_pct_before.append(p2_last3_pct)\n",
    "\n",
    "    p1_career_wins_last5_pct_before.append(p1_last5_pct)\n",
    "    p2_career_wins_last5_pct_before.append(p2_last5_pct)\n",
    "\n",
    "    p1_career_wins_last10_pct_before.append(p1_last10_pct)\n",
    "    p2_career_wins_last10_pct_before.append(p2_last10_pct)\n",
    "\n",
    "    \n",
    "    # Update total wins\n",
    "    career_wins_dict[p1] += 1\n",
    "\n",
    "    # Update recent win/loss history\n",
    "    career_wins_history_dict[p1].append(1)  # p1 won\n",
    "    career_wins_history_dict[p2].append(0)  # p2 lost\n",
    "\n",
    "# Assign to DataFrame\n",
    "df_subset['p1_career_wins_before'] = p1_career_wins_before\n",
    "df_subset['p2_career_wins_before'] = p2_career_wins_before\n",
    "df_subset['p1_career_wins_last3_pct_before'] = p1_career_wins_last3_pct_before\n",
    "df_subset['p2_career_wins_last3_pct_before'] = p2_career_wins_last3_pct_before\n",
    "df_subset['p1_career_wins_last5_pct_before'] = p1_career_wins_last5_pct_before\n",
    "df_subset['p2_career_wins_last5_pct_before'] = p2_career_wins_last5_pct_before\n",
    "df_subset['p1_career_wins_last10_pct_before'] = p1_career_wins_last10_pct_before\n",
    "df_subset['p2_career_wins_last10_pct_before'] = p2_career_wins_last10_pct_before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ae2b9-a927-4109-936b-361d87874ea0",
   "metadata": {},
   "source": [
    "# Removing Anomalous Data after ELOs and Match Histories have been Calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8159d04-63d0-4914-a53b-d472ca22ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values for height, surface and age columns\n",
    "df_subset = df_subset.dropna(subset=['p1_height'])\n",
    "df_subset = df_subset.dropna(subset=['p2_height'])\n",
    "df_subset = df_subset.dropna(subset=['surface'])\n",
    "df_subset = df_subset.dropna(subset=['p1_age'])\n",
    "df_subset = df_subset.dropna(subset=['p2_age'])\n",
    "\n",
    "# Remove rows with unknown player handedness\n",
    "df_subset = df_subset[(df_subset['p1_hand'] != 'U') & (df_subset['p2_hand'] != 'U') & (df_subset['p2_hand'] != 'A')]\n",
    "\n",
    "#List of anomalous players to remove\n",
    "anom_heights_list = ['Jorge Brian Panta Herreros',\n",
    "'Johannes Ingildsen',\n",
    "'Viacheslav Bielinskyi',\n",
    "'Kooros Ghasemi',\n",
    "'Alexander Stater',                     \n",
    "'William Grant',\n",
    "'Ilija Vucic',\n",
    "'Andrew Rogers'\n",
    "]\n",
    "\n",
    "# remove anomalous players\n",
    "mask = df_subset['p1_name'].isin(anom_heights_list) | df_subset['p2_name'].isin(anom_heights_list)\n",
    "df_subset = df_subset.drop(df_subset[mask].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bb8f9-4528-4e96-8b0e-7e490186c361",
   "metadata": {},
   "source": [
    "###### Checking for any NaN values or anomolous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e948344-1bd6-43bb-8e2a-0c2fa15f9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displayes number of NaN values for each column\n",
    "df_subset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fea57a-3b86-4520-884e-01c7e9d340fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee2440-6a71-4ad0-b15c-4089f93325f8",
   "metadata": {},
   "source": [
    "# Reset Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f0bcf-0939-46e5-a282-5828000e1a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_subset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8fced-e856-41dd-a27c-d62360df449a",
   "metadata": {},
   "source": [
    "# Creating Target Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6718aee5-d17e-4354-8524-b5acec9acb5f",
   "metadata": {},
   "source": [
    "###### In the data I am using, the winner of the match is always player 1. To model this data, the winner must be randomly player 1 or player 2 and a new 'result' feature needs to be added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc9885-0ccb-4a68-a801-4d152f0a02e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make a copy so original data isn't affected\n",
    "df_balanced = df_subset.copy()\n",
    "\n",
    "# Add a result column and make the value equal to 1 because player 1 always wins\n",
    "df_balanced[\"result\"] = 1\n",
    "\n",
    "# Randomly choose half of the rows to swap player1 and player2\n",
    "swap_mask = np.random.rand(len(df_subset)) < 0.5\n",
    "\n",
    "# Swap player-related columns where swap_mask is True\n",
    "cols_to_swap = [\n",
    "    ('p1_id', 'p2_id'),\n",
    "    ('p1_hand', 'p2_hand'),\n",
    "    ('p1_name', 'p2_name'),\n",
    "    ('p1_age', 'p2_age'),\n",
    "    ('p1_height', 'p2_height'),\n",
    "    ('p1_age_diff', 'p2_age_diff'),\n",
    "    ('p1_height_diff', 'p2_height_diff'),\n",
    "    ('p1_h2h_wins', 'p2_h2h_wins'),\n",
    "    ('p1_h2h_wins_before_total_diff', 'p2_h2h_wins_before_total_diff'),\n",
    "    ('p1_h2h_wins_before_last1_diff', 'p2_h2h_wins_before_last1_diff'),\n",
    "    ('p1_h2h_wins_before_last2_diff', 'p2_h2h_wins_before_last2_diff'),\n",
    "    ('p1_h2h_wins_before_last3_diff', 'p2_h2h_wins_before_last3_diff'),\n",
    "    ('p1_h2h_wins_before_last4_diff', 'p2_h2h_wins_before_last4_diff'),\n",
    "    ('p1_h2h_wins_before_last5_diff', 'p2_h2h_wins_before_last5_diff'),\n",
    "    ('p1_h2h_wins_before_last10_diff', 'p2_h2h_wins_before_last10_diff'),\n",
    "    ('p1_elo_before', 'p2_elo_before'),\n",
    "    ('p1_elo_diff_before', 'p2_elo_diff_before'),\n",
    "    ('p1_surface_elo_before', 'p2_surface_elo_before'),\n",
    "    ('p1_surface_elo_diff_before', 'p2_surface_elo_diff_before'),\n",
    "    ('p1_total_matches_played_before', 'p2_total_matches_played_before'),\n",
    "    ('p1_total_matches_played_before_diff', 'p2_total_matches_played_before_diff'),\n",
    "    ('p1_career_wins_before', 'p2_career_wins_before'),\n",
    "    ('p1_career_wins_last3_pct_before', 'p2_career_wins_last3_pct_before'),\n",
    "    ('p1_career_wins_last5_pct_before', 'p2_career_wins_last5_pct_before'),\n",
    "    ('p1_career_wins_last10_pct_before', 'p2_career_wins_last10_pct_before'),\n",
    "    ('p1_elo_rolling_last5', 'p2_elo_rolling_last5'),\n",
    "    ('p1_elo_rolling_last10', 'p2_elo_rolling_last10'),\n",
    "    ('p1_elo_rolling_last20', 'p2_elo_rolling_last20'),\n",
    "    ('p1_elo_rolling_last5_diff_before', 'p2_elo_rolling_last5_diff_before'),\n",
    "    ('p1_elo_rolling_last10_diff_before', 'p2_elo_rolling_last10_diff_before'),\n",
    "    ('p1_elo_rolling_last20_diff_before', 'p2_elo_rolling_last20_diff_before'),\n",
    "    ('p1_surface_elo_rolling_last5', 'p2_surface_elo_rolling_last5'),\n",
    "    ('p1_surface_elo_rolling_last10', 'p2_surface_elo_rolling_last10'),\n",
    "    ('p1_surface_elo_rolling_last20', 'p2_surface_elo_rolling_last20'),\n",
    "    ('p1_surface_elo_rolling_last5_diff_before', 'p2_surface_elo_rolling_last5_diff_before'),\n",
    "    ('p1_surface_elo_rolling_last10_diff_before', 'p2_surface_elo_rolling_last10_diff_before'),\n",
    "    ('p1_surface_elo_rolling_last20_diff_before', 'p2_surface_elo_rolling_last20_diff_before')\n",
    "]\n",
    "\n",
    "# Loop through each pair of columns and swap values for the selected rows\n",
    "for col1, col2 in cols_to_swap:\n",
    "    temp = df_balanced.loc[swap_mask, col1].copy()\n",
    "    df_balanced.loc[swap_mask, col1] = df_balanced.loc[swap_mask, col2]\n",
    "    df_balanced.loc[swap_mask, col2] = temp\n",
    "\n",
    "# Update the result column to 0 where players were swapped\n",
    "df_balanced.loc[swap_mask, \"result\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be64db-37e6-4ac3-8bb4-fe0cece8f55a",
   "metadata": {},
   "source": [
    "# Removing Redundant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b1d2b-07af-4bff-8999-03384737429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_features = [\n",
    "    'tournament_name',\n",
    "    'p1_id',\n",
    "    'p2_id',\n",
    "    'p2_age_diff',\n",
    "    'p2_height_diff',\n",
    "    'p2_h2h_wins_before_total_diff',\n",
    "    'p2_h2h_wins_before_last1_diff',\n",
    "    'p2_h2h_wins_before_last2_diff',\n",
    "    'p2_h2h_wins_before_last3_diff',\n",
    "    'p2_h2h_wins_before_last4_diff',\n",
    "    'p2_h2h_wins_before_last5_diff',\n",
    "    'p2_h2h_wins_before_last10_diff',\n",
    "    'p2_elo_diff_before',\n",
    "    'p2_surface_elo_diff_before',\n",
    "    'p2_total_matches_played_before_diff',\n",
    "    'p2_career_wins_last3_pct_before',\n",
    "    'p2_career_wins_last5_pct_before',\n",
    "    'p2_career_wins_last10_pct_before'\n",
    "]\n",
    "\n",
    "# Drop them from the DataFrame\n",
    "df_balanced = df_balanced.drop(redundant_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ac5fa-1ff5-440c-9b38-6037320cc79c",
   "metadata": {},
   "source": [
    "# Encoding Players Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5fbda-0504-40af-af20-c1d967b30a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Collect all players names \n",
    "all_players = pd.concat([df_balanced['p1_name'], df_balanced['p2_name']])\n",
    "\n",
    "# Fit a LabelEncoder on all names\n",
    "le_players = LabelEncoder()\n",
    "le_players.fit(all_players)\n",
    "\n",
    "# Transform player name columns to player id columns\n",
    "df_balanced['p1_id'] = le_players.transform(df_balanced['p1_name'])\n",
    "df_balanced['p2_id'] = le_players.transform(df_balanced['p2_name'])\n",
    "\n",
    "# Encode player hand\n",
    "le_hand = {'R': 0, 'L': 1}\n",
    "df_balanced['p1_hand_id'] = df_balanced['p1_hand'].map(le_hand)\n",
    "df_balanced['p2_hand_id'] = df_balanced['p2_hand'].map(le_hand)\n",
    "df_balanced['p1_hand_id'] = df_balanced['p1_hand_id'].astype(int)\n",
    "df_balanced['p2_hand_id'] = df_balanced['p2_hand_id'].astype(int)\n",
    "\n",
    "# Encode surface\n",
    "le_surface = LabelEncoder()\n",
    "df_balanced['surface_id'] = le_surface.fit_transform(df_balanced['surface'])\n",
    "\n",
    "# Encode tournament level\n",
    "le_tournament_level = LabelEncoder()\n",
    "df_balanced['tournament_level_id'] = le_tournament_level.fit_transform(df_balanced['tournament_level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d903709c-15ad-4509-b38a-a0ee35b9b9fd",
   "metadata": {},
   "source": [
    "# Reordering Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce336532-ed3b-47d2-8279-e3a9f551e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_new_order = [\n",
    "    'tournament_date',\n",
    "    'tournament_level_id',\n",
    "    'surface_id',\n",
    "    'p1_id',\n",
    "    'p1_hand_id',\n",
    "    'p2_id',\n",
    "    'p2_hand_id',\n",
    "    'p1_age_diff',\n",
    "    'p1_height_diff',\n",
    "    'p1_h2h_wins_before_total_diff',\n",
    "    'p1_elo_diff_before',\n",
    "    'p1_elo_rolling_last5_diff_before',\n",
    "    'p1_elo_rolling_last10_diff_before',\n",
    "    'p1_elo_rolling_last20_diff_before',\n",
    "    'p1_surface_elo_diff_before',\n",
    "    'p1_surface_elo_rolling_last5_diff_before',\n",
    "    'p1_surface_elo_rolling_last10_diff_before',\n",
    "    'p1_surface_elo_rolling_last20_diff_before',\n",
    "    'p1_total_matches_played_before_diff',\n",
    "    'result'\n",
    "]\n",
    "df_balanced = df_balanced[cols_new_order].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f90ca1-ac01-4c69-a3d5-50f75cca21c3",
   "metadata": {},
   "source": [
    "# Saving Dataframe as a new .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92299e8d-6d57-41f5-addd-d4171b5b4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a47696-364c-44b6-b4f9-85fd8478157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.to_csv('feature_engineered_tennis_data.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_xpu)",
   "language": "python",
   "name": "torch_xpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
